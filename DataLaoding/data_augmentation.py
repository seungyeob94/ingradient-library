import timeimport osimport SimpleITK as sitkimport numpy as npimport torchimport torch.nn.functional as Fimport torch.nn as nnimport randomclass Affine_3D(object):    """     Input Images's shape => (n_modalities, n_x, n_y, n_z)    *추가 구현해야 할 사항    현재: 한 축에 대해서만 Rotation 시키기 때문에 여러 축에 대해 회전시키게 되면 Vectorization이 깨짐.    Input:    axis = [회전 시킬 축들 순서 이미지가 (n, x, y, z)라면, 0: x, 1: y, 2: z]    degree = [axis 순서에 맞게 회전시킬 정도, axis = [0, 1, 2]고 degree = [np.pi, np.pi, np.pi]면 모든 축을 기준으로 180도 회전]        후에 여러 축에 대한 Scaliing 연산에서도 Vectorization 연산 가능하게끔 변경    계산 복잡도 줄이기 위한 방법    => 회전 행렬 곱을 없애고 만드는 수식으로 결정 (DP Approaches) (연산 비교 Before DP & After DP)        """    def __init__(self, degree = None, axis = [0,1,2], scale = None, is_random = False, use_gpu = True, is_3d_img = True, device = None, cal_time = True):        """        is_random = True 일 경우, degree와 axis는 [min, max] 형태로 표현해야한다.        """        self.use_gpu = use_gpu        self.transform_matrices = []        if is_random:            if degree:                degree = [random.uniform(-d, d) for d in degree]             if scale:                scale = random.uniform(scale[0], scale[1])                for a, d in zip(axis, degree):            self.transform_matrices.append(self.set_rotate_matrix(a, d))#Rotation Matrix 생성        if scale:            self.transform_matrices.append(self.set_scale_matrix(scale))        if self.use_gpu:            self.device = device        self.cal_time = cal_time    def __call__(self, images):        start_time = time.time()        n_modalities, n_x, n_y, n_z = images.shape        img_size = [n_x,n_y,n_z]        coords = self.create_coords(img_size) #Coordinates 생성        coords_bound = np.array(img_size).reshape(-1,1)-1            if not self.use_gpu :            coords = coords.numpy()            coords = coords - ((np.array(img_size)-1)/2).reshape(3,1,1,1)            before_shape = coords.shape            for i in range(len(self.transform_matrices)-1): #Associative Property                self.transform_matrices[i+1] = np.dot(self.transform_matrices[i], self.transform_matrices[i+1])             coords = np.dot(self.transform_matrices[-1], coords.reshape(3, -1)).reshape(*before_shape) #Rotate Coordinates            coords = coords + ((np.array(img_size)-1)/2).reshape(3,1,1,1)  #Reverse Coordinates => 이미지 중앙 중심에서 왼쪽 상단 중심으로            reshaped = np.rint(coords.reshape(3, -1)).astype(int) #Mapping 해주는 함수            reshaped = np.clip(reshaped, [[0], [0], [0]], coords_bound)            images = images[:, reshaped[0], reshaped[1], reshaped[2]]            images = images.reshape(n_modalities, n_x, n_y, n_z)                            else:            before_shape = coords.shape            coords = coords.to(self.device).double()            half_img_size = ((torch.Tensor(img_size).to(self.device) - 1)/2).reshape(3,1,1,1)            coords = coords - half_img_size            self.transform_matrices = torch.Tensor(self.transform_matrices).to(self.device).double()            for i in range(len(self.transform_matrices)-1):                self.transform_matrices[i+1] = torch.matmul(self.transform_matrices[i], self.transform_matrices[i+1])            coords = torch.matmul(self.transform_matrices[-1], coords.reshape(3, -1)).reshape(*before_shape)            coords = coords + half_img_size            reshaped = coords.reshape(3, -1).round() #Mapping 해주는 함수            reshaped = torch.clamp(reshaped, torch.Tensor([[0], [0], [0]]).to(0).int(), torch.Tensor(coords_bound).to(0).int()).long()            images = torch.from_numpy(images).to(0)            images = images[:, reshaped[0], reshaped[1], reshaped[2]]            images = images.reshape(n_modalities, n_x, n_y, n_z).cpu()        map_coords_time = time.time()                    return images            def set_rotate_matrix(self, axis, degree):        function_list = [self.rotation_x_3d, self.rotation_y_3d, self.rotation_z_3d]        return function_list[axis](degree)    def set_scale_matrix(self, scale):        return np.eye(3) * 1/scale    def rotation_x_3d(self, angle):        return np.array([[1, 0, 0], [0, np.cos(angle), -np.sin(angle)], [0, np.sin(angle), np.cos(angle)]])    def rotation_y_3d(self, angle):        return np.array([[np.cos(angle), 0, np.sin(angle)], [0, 1, 0], [-np.sin(angle), 0, np.cos(angle)]])    def rotation_z_3d(self, angle):        return np.array([[np.cos(angle), -np.sin(angle), 0], [np.sin(angle), np.cos(angle), 0], [0, 0, 1]])    def create_coords(self, img_shape):        tmp = tuple([torch.arange(i) for i in img_shape])        grid_x, grid_y, grid_z = torch.meshgrid(*tmp)        return torch.vstack((grid_x.unsqueeze_(0), grid_y.unsqueeze_(0), grid_z.unsqueeze_(0)))            class Gaussian_noise(object):    """    np.random.uniform 으로 4개 만든 후    넘는 indexing에 대해 gaussian random normal 사용    modularity    10% 정도 빠르다.    """    def __init__(self, noise_variance = 0.1, probability_per_channel = 0.5 , use_gpu = False, device = None, random_per_modularities = False):                self.random_per_modularities = random_per_modularities        self.noise_variance = noise_variance        self.use_gpu = use_gpu        self.device = device        self.probability_per_channel = probability_per_channel            def __call__(self, images):        """        Images = (N_modularities, X, Y, Z)        """        if self.random_per_modularities:            self.augment_modularities = np.where(np.random.uniform(size = 4) > self.probability_per_channel)[0].astype(int)            _, X, Y, Z = images.shape            new_size = (len(self.augment_modularities), X, Y, Z)            if len(self.augment_modularities) < 1:                return images                else:            new_size = images.shape        noise_array = torch.normal(mean = torch.zeros(new_size), std = self.noise_variance).float().to(self.device)        images = torch.from_numpy(images).float().to(self.device)                if self.use_gpu:            if self.random_per_modularities:                self.augment_modularities = torch.from_numpy(self.augment_modularities).to(self.device)                if self.random_per_modularities:            images[self.augment_modularities,: ,:, :] = images[self.augment_modularities,: ,:, :] + noise_array                else:            images = images + noise_array        return imagesclass Brightness(object):      def __init__(self, device = None, mu = 0.0, sigma = 1.0):    self.device = device    self.mu = mu    self.sigma = sigma      def __call__(self, images):    n_modalities, _, _, _ = images.shape    noise_array = torch.normal(mean = torch.Tensor([self.mu]).repeat(n_modalities), std = self.sigma).float()    noise_array = noise_array.view(n_modalities, 1,1,1).to(self.device)    images = torch.from_numpy(images).to(self.device)    return images - noise_arrayclass Contrast(object):      def __init__(self, device = None, contrast_range = [0.75, 12.25], preserve_range = True):    self.device = device    self.preserve_range = preserve_range    self.contrast_range = contrast_range      def __call__(self, images):    n_modalities, nx, ny, nz = images.shape    if np.random.random() < 0.5 and self.contrast_range[0] < 1:        factor = torch.FloatTensor(n_modalities,1,1,1).uniform_(self.contrast_range[0], 1).to(self.device)    else:        factor = torch.FloatTensor(n_modalities,1,1,1).uniform_(max(self.contrast_range[0], 1), self.contrast_range[1]).to(self.device)    images = torch.from_numpy(images).to(self.device).float()    mmean = torch.mean(images.view(n_modalities, -1), 1).view(n_modalities, 1, 1, 1)    mmax = torch.max(images.view(n_modalities, -1), dim = 1)[0].view(n_modalities, 1, 1, 1)    mmin = torch.min(images.view(n_modalities, -1), dim = 1)[0].view(n_modalities, 1, 1, 1)    images = (images - mmean ) * factor + mmean    if self.preserve_range:        images = torch.where(images > mmax, mmax, images)        images = torch.where(images < mmin, mmin, images)    return images    class GammaTransform():        def __init__(self, gamma_range = (0.5, 2), epsilon = 1e-7, device = None, retain_stats = False):        self.gamma_range = gamma_range        self.device = device        self.epsilon = epsilon        self.retain_stats = retain_stats    def __call__(self, images):        n_modalities, nx, ny, nz = images.shape        images = torch.from_numpy(images).to(self.device).float()        if np.random.random() < 0.5 and self.gamma_range[0] < 1:            gamma = torch.FloatTensor(n_modalities,1,1,1).uniform_(self.gamma_range[0], 1).to(self.device)        else:            gamma = torch.FloatTensor(n_modalities,1,1,1).uniform_(max(self.gamma_range[0], 1), self.gamma_range[1]).to(self.device)                if self.retain_stats:            before_mmean = torch.mean(images.view(n_modalities, -1), 1).view(n_modalities, 1, 1, 1)            before_mstd = torch.std(images.view(n_modalities, -1), 1).view(n_modalities, 1, 1, 1)        mmax = torch.max(images.view(n_modalities, -1), dim = 1)[0].view(n_modalities, 1, 1, 1)        mmin = torch.min(images.view(n_modalities, -1), dim = 1)[0].view(n_modalities, 1, 1, 1)                mrange = mmax - mmin         temp = (mrange + self.epsilon)                images = torch.pow( ((images - mmin) / temp), gamma) * temp + mmin                if self.retain_stats:            after_mmean = torch.mean(images.view(n_modalities, -1), 1).view(n_modalities, 1, 1, 1)            after_mstd = torch.std(images.view(n_modalities, -1), 1).view(n_modalities, 1, 1, 1)            images = images - after_mmean            images = images / (after_mstd + 1e-8) * before_mstd            images = images + before_mmean                return images                